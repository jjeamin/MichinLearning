## MichinLearning 스터디 동아리입니다: Machine Learning을 함께 공부합니다!
 
 ![미친러닝](./michin.jpg)
 딥러닝을 통해서 생성된 합성 이미지 ! 피카소풍의 사진! 
*  *  *
 
 ![미친 러닝](./crazy.png)
# + 수퍼 대학원생 김용휘!
 
 ### Open Mind ! Open Source !

```미친 러닝``` 스터디 그룹입니다. 한림대학교 소프트웨어융합대학 학부생 5명과 대학원생 1명 박사후 연구원 1명 및 교수 1인으로 구성된 모임입니다.
오픈소스 SW를 활용하여 시대의 큰 흐름 중 하나인 인공지능/기계학습 등에 대해서 함께 공부하고 토의합니다.

*  *  *

1차적으로 모임에서는 김성훈 교수의 모두의 딥러닝을 시청후 모여 다양한 토론을 진행합니다.

   - [모두를 위한 머신러닝/딥러닝 강의](https://hunkim.github.io/ml/)
   - 추가적인 동영상 참고자료 ! [테리의 딥러닝 토크](https://www.youtube.com/watch?v=D4zqigCb8co&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)

   - 임승현: [ML Book](https://github.com/jeonggunlee/MichinLearning/blob/master/Machine-Learning.pdf)
   
   - 임병준: 실습시 GPU 사용을 통해 빠른 learning을 하고 싶다면 --> [CLICK](https://github.com/jeonggunlee/MichinLearning/blob/master/tensorflow-gpu%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.txt)
   
   - [데이터 사이언티스트 인터뷰 질문 모음](https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/)
   - [비트폭 최적화 관련 자료: Quantization](https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd)
   
*  *  *

**1차 모임: 9월 6일 목요일**: 함께 동영상 시청 - 이후 모임에서는 시청후 토론 방식으로 진행하는 것으로 정함

*  *  *

9월 10일 - "학과 세미나 참가" : 4차산업혁명과 오픈소스 기반 머신러닝 기술 by 최권택 (강남대학교)

*  *  *

**2차 모임: 9월 13일 목요일**

잘문1) training set, validation set, 그리고 test set의 차이

   - 참조: [#1.1. Training / Test / Validation Set : 오버피팅을 피하는 방법](https://www.youtube.com/watch?v=GtLe9Z2No28&index=7&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)

     - validation test set과의 차이점은 test set은 모델의 '최종 성능' 을 평가하기 위해서 쓰이며, training의 과정에 관여하지 않는 차이가 있습니다. 반면 validation set은 여러 모델 중에서 최종 모델을 선정하기 위한 성능 평가에 관여한다 보시면됩니다. 따라서 validation set은 training과정에 관여하게 됩니다. 즉, validation set은 training 과정에 관여를 하며, training이 된 여러가지 모델 중 가장 좋은 하나의 모델을 고르기 위한 셋입니다. test set은 모든 training 과정이 완료된 후에 최종적으로 모델의 성능을 평가하기 위한 셋입니다. 만약 test set이 모델을 개선하는데 쓰인다면, 그건 test set이 아니라 validation set입니다. 만약 여러 모델을 성능 평가하여 그 중에서 가장 좋은 모델을 선택하고 싶지 않은 경우에는 validation set을 만들지 않아도 됩니다. 하지만 이 경우에는문제가 생길 것입니다. (test accuracy를 예측할 수도 없고, 모델 튜닝을 통해 overfitting을 방지할 수도 없습니다.) [출처](http://3months.tistory.com/118 [Deep Play])
     
     - training 과정에 관여한다는 것이 성능을 평가한다는 것이지, 모델의 수정하는데 사용한다는 말은 아닌것 같네요!
    
   - [참조](https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set):
    
     - The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model. Ideally, the test set should be kept in a “vault,” and be brought out only at the end of the data analysis.

*  *  *
**3차 모임: 9월 17일 월요일**

잘문1) cross-entropy를 cost 함수로 활용하는 이유 ? 그리고 cross-entropy와 Mean squre error가 같다고(?)하던데 어떻게 같은건가라는 질문에 답을 해줄수 있는 글인것 같네요. 한번 읽어 보면 좋을 것 같네요.
   - [Neural Networks의 학습속도 저하(Slowdown)를 막는 Cross-Entropy Cost Function](http://solarisailab.com/archives/2237)
   - [Cross Entropy의 정확한 확률적 의미](https://taeoh-kim.github.io/blog/cross-entropy%EC%9D%98-%EC%A0%95%ED%99%95%ED%95%9C-%ED%99%95%EB%A5%A0%EC%A0%81-%EC%9D%98%EB%AF%B8/)
   - [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)

>[재밌는 아이디어] 사실 어디 회사분이 아이들이 아빠 엄마를 인식하는 것과 유사하게 아빠 사진 엄마 사진 등이 캡쳐될때마다 (아빠라는 소리가 들릴때 카메라를 찍어서 찍힌 사진에서 얼굴을 검출하고, 아빠라고 labelling 함) online learning을 해서 사람들을 인식해가는 걸 만들어보고 싶다고 했는데...9장이 이런 부분에 사용될 수 있을 것 같네요. 클라우드도 사용하구~ ㅋㅋ


*  *  *
**2018년 9월 20일**

질문1) 딥모델을 구성할때, layer를 증가시키는 거와 layer에서 node / weight를 증가시키는 것에 대한 효과 ?
어떤 것이 더 효과적일까 ?
   - 이미지의 경우 layer의 노드수가 중요하지 않을까?
   - 근데 왜 최근 모델은 layer에 집중할까?
   - http://playground.tensorflow.org 시뮬레이션 해보면 어떨까?
   - [Stackoverflow](https://stackoverflow.com/questions/35520587/how-to-determine-the-number-of-layers-and-nodes-of-a-neural-network)
      > As Yoshua Bengio, Head of Montreal Institute for Learning Algorithms remarks:
      > "Very simple. Just keep adding layers until the test error does not improve anymore."
      >
      >
      > A method recommended by Geoff Hinton is to add layers until you start to overfit your training set. Then you add dropout or another regularization method.

질문2) Back-Propagation할때 0이 나오면 어떻게 처리하는가?
       if( error == 0 ) error = MIN;

*  *  *
**2018년 9월 27일** : **Convolutional Neural Networks**

**질문1) pooling을 쓰는 이유?**

   - Overfitting 을 방지!
   -> 이미지를 계속 필터로 만들다보면 특징의 수가 계속계속 기하 급수적으로 증가하기 때문에 overfitting의 위험이 있습니다.
       그렇기 때문에 중간에 pooling을 해준다면 개수가 적절하게 유지되어서 overfitting을 최대한 방지한다.
       참고사이트 : [링크](https://mc.ai/cnn%EC%97%90%EC%84%9C-pooling%EC%9D%B4%EB%9E%80/)

**질문 2) Max pooling을 왜 쓰는 건가?**
   
> [재민 코멘트] 일단 pooling이란 *계산 복잡도를 줄이기위해서* 사용합니다. 그렇다면 평균(average)와 최대값(max value)중에 어울리는 단어는 무엇일까요? 일반적으로 복잡한 것을 간단하게 만들기 위해서 숫자들에게 구분을 지어야 한다면 당연하게 max와 잘 어울린다고 생각하게 됩니다.  그러나 둘중 어느것이 좋다고 정확히 말할 수는 없습니다. 왜냐하면 max pooling은 중요한 부분(가장자리)을 추출하는데 특화(몇가지중 숫자 1개만 선택) 되어있고 average pooling은 부드럽고 유연한 추출을 하는데 특화(모두를 평균내어서 선택)되어 있기 때문입니다. 즉, *case에 따라 다르다는 것*입니다. 그러나 저희는 보통 이미지를 구분하는데 일반적으로 극단적인 특징을 요구한다고 합니다. 그렇기 때문에 max pooling을 사용하는 것이 아닐까요?? 결론적으로 average 보다 보편화된 max가 이유가 아닐까요?

[링크](https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling) <- 여기에 max 와 average의 차이를 보여주는 이미지가 있습니다. 그리고 여기서 참고했습니다.

질문 3) CNN이 왜 좋지 ? [링크](http://aikorea.org/cs231n/convolutional-networks/)

>컨볼루셔널 레이어 (이하 CONV) 

>CONV 레이어는 ConvNet을 이루는 핵심 요소이다. CONV 레이어의 출력은 3차원으로 정렬된 뉴런들로 해석될 수 있다. 이제부터는 뉴런들의 연결성 (connectivity), 그들의 공간상의 배치, 그리고 모수 공유(parameter sharing) 에 대해 알아보자.

>

>개요 및 직관적인 설명. CONV 레이어의 모수(parameter)들은 일련의 학습가능한 필터들로 이뤄져 있다. 각 필터는 가로/세로 차원으로는 작지만 깊이 (depth) 차원으로는 전체 깊이를 아우른다. 포워드 패스 (forward pass) 때에는 각 필터를 입력 볼륨의 가로/세로 차원으로 슬라이딩 시키며 (정확히는 convolve 시키며) 2차원의 액티베이션 맵 (activation map)을 생성한다. 필터를 입력 위로 슬라이딩 시킬 때, 필터와 입력의 요소들 사이의 내적 연산 (dot product)이 이뤄진다. 직관적으로 설명하면, 이 신경망은 입력의 특정 위치의 특정 패턴에 대해 반응하는 (activate) 필터를 학습한다. 이런 액티베이션 맵 (activation map)을 깊이 (depth) 차원을 따라 쌓은 것이 곧 출력 볼륨이 된다. 그러므로 출력 볼륨의 각 요소들은 입력의 작은 영역만을 취급하고, 같은 액티베이션 맵 내의 뉴런들은 같은 모수들을 공유한다 (같은 필터를 적용한 결과이므로). 이제 이 과정에 대해 좀 더 깊이 파헤쳐보자.

>

>로컬 연결성 (Local connectivity). 이미지와 같은 고차원 입력을 다룰 때에는, 현재 레이어의 한 뉴런을 이전 볼륨의 모든 뉴런들과 연결하는 것이 비 실용적이다. 대신에 우리는 레이어의 각 뉴런을 입력 볼륨의 로컬한 영역(local region)에만 연결할 것이다. 이 영역은 리셉티브 필드 (receptive field)라고 불리는 초모수 (hyperparameter) 이다. 깊이 차원 측면에서는 항상 입력 볼륨의 총 깊이를 다룬다 (가로/세로는 작은 영역을 보지만 깊이는 전체를 본다는 뜻). 공간적 차원 (가로/세로)와 깊이 차원을 다루는 방식이 다르다는 걸 기억하자.


질문 4) 필터의 사이즈는 어떻게 잡으면 좋을까 ? 크게 작게 ????

**2018년 10월 1일** : **Recurrent Neural Networks**

특별한 질문은 없고, 향후 모임 진행 방향 토의! : 개별 연구 진행!

**2018년 10월 4일** : 개별 관심/연구 사항 토의 미팅

    - 임병준: Gradient Descent 방법 개별연구
    - 이민정: Self Review
    - 임승현: Review
    - 정재민: Crawled data analysis
    - 이종학: Machine Learning Math.
    
    
**2018년 10월 8일** : 인공지능 세미나 참석

    - 주제 : 4차산업혁명을 선도하는 인공지능 최신기술 동향
    - 연사 : 변혜란 교수(연세대학교)
    

**2018년 10월 15일** : 오픈소스 SW 세미나 

    - 주제 : 4차산업시대에 필요한 소프트웨어 인재상
    - 연사 : 손기성 수석(삼성전자)

*  *  *

**2018년 10월 24일** : 대한 전공공학회 논문 작성 !!!!!!!

### 딥-뉴럴 네트워크의 파라미터 최적화: MNIST 사례분석
### 김민정, 임병준, 임승현, 정재민, 이종학, 이정근 (한림대학교 소프트웨어 융합대학)

> In order to deploy a state-of-art modern deep learning algorithm in an embedded system, it is essential to minimize a deep learning model. Particularly, the parameters such as weights and biases consume significant amount of a system memory. Consequently, we need to minimize the memory size of those model parameters to deploy a deep learning algorithm in a resource limited embedded system. In this paper, we investigate an memory size optimization of the deep learning model for the MNIST case, a most comprehensible and widely used deep learning example in order to find an impact of bit-width optimization for the weights and biases with considering their numerical format, a standard IEEE 32 bit floating format.

![paper](https://github.com/jeonggunlee/MichinLearning/blob/master/paper.JPG)

> 본 연구는 과학기술 정보통신부 및 정보통신기술 진흥 센터의 “소프트웨어 중심대학 지원사업(2018-0-00216)”의 연구결과로 수행되었습니다. 더불어, 딥러닝을 함께 연구하면서 도움을 준 한림대학교 컴퓨터공학과 석사과정 김용휘님께 감사합니다.

*  *  *

**2018년 10월 31일** : 대한 전공공학회 논문 선정!

![accept](https://github.com/jeonggunlee/MichinLearning/blob/master/accept.png)

*  *  *

**2018년 11월 1일** : Research Meetup: 앞으로의 논문 발표 계획등에 대한 토의

![meetup](https://github.com/jeonggunlee/MichinLearning/blob/master/meetup.png)

*  *  *

**2018년 11월 7일-8일** : [Nvidia AI Conference](https://www.nvidia.com/ko-kr/ai-conference/) 참가~ 
    - NVIDIA AI CONFERENCE SEOUL | 7 - 8 November COEX CONVENTION

### 첫째 날
![AI Conference 참관기](https://github.com/jeonggunlee/MichinLearning/blob/master/ai_conf.jpg)

### 둘째 날
![AI Conference 참관기](https://github.com/jeonggunlee/MichinLearning/blob/master/ai_conf2.jpg)


*  *  *

**2018년 11월 13일: 논문 발표**</br>
    - 임병준: CNN을 이용한 글자별 한글 필적감정 알고리즘([논문](./paper/CNN%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EA%B8%80%EC%9E%90%EB%B3%84%20%ED%95%9C%EA%B8%80%20%ED%95%84%EC%A0%81%EA%B0%90%EC%A0%95%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98.pdf) / [발표자료](./paper/CNN%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EA%B8%80%EC%9E%90%EB%B3%84%20%ED%95%9C%EA%B8%80%20%ED%95%84%EC%A0%81%EA%B0%90%EC%A0%95%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C(%EC%9E%84%EB%B3%91%EC%A4%80).pptx))</br>
    - 임승현: 머신러닝을_이용한_표정분류

*  *  *

**2018년 11월 15일: 논문 발표**</br>
    - 김민정: 
    - 이종학: CNN을 이용한 문자열 CAPTCHA 공격

*  *  *

**2018년 11월 19일: 논문 발표**</br>
    - 정재민: Parallel Stacked Bidirectional LSTM 모델을 이용한 한국어 영화리뷰 감성 분석

- [논문자료](./paper/Parallel%20Stacked%20Bidirectional%20LSTM%20%EB%AA%A8%EB%8D%B8%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%98%81%ED%99%94%EB%A6%AC%EB%B7%B0%20%EA%B0%90%EC%84%B1%20%EB%B6%84%EC%84%9D.pdf)   
- [PPT](./paper/Parallel%20Stacked%20Bidirectional%20LSTM%20%EB%AA%A8%EB%8D%B8%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%98%81%ED%99%94%EB%A6%AC%EB%B7%B0%20%EA%B0%90%EC%84%B1%20%EB%B6%84%EC%84%9D.pptx)
*  *  *

**2018년 11월 23일-24일**
    - 대한전자공학회 2018년도 정기총회 및 추계학술대회
    - 2018년 11월 23일(금)~24일(토) / 송도 컨벤시아(인천광역시 연수구 송도동)

![논문발표](./paper_mj.jpg)    

**[추천] 2018년 12월 7일 (금요일)**

    - 2018 한국정보과학회 학술심포지움 유비쿼터스 컴퓨팅과 웹 정보기술, 12월 7일(금), 부경대학교
    - http://db.pknu.ac.kr/ucwit2018/?call_for_papers
    - 논문접수 마감: 2018년 11월 16일 (금)

**[추천] 2018년 12월 19일(수) ~ 21일(금) (금요일)**

    - 한국정보과학회 KSC2018 - 2018 한국소프트웨어종합학술대회 (Korea Software Congress 2018)
    - 2018년 12월 19일(수) ~ 21일(금), 평창 휘닉스파크
    - 논문접수마감(일반논문/학부생주니어논문경진대회): 10월 22일(월)

 
*  *  *
### 참여자 리스트입니다.
- 소프트웨어 융합 대학 교수 이정근 [http://www.onchip.net](http://www.onchip.net)
- 컴퓨터공학과 대학원 Nguyen Van Toan 박사
- 컴퓨터공학과 대학원 김용휘 (석사과정)
- 소프트웨어 융합 대학 학부생: 임병준 (3학년), 임승현 (3학년), 이종학 (3학년), 정재민 (3학년), 김민정 (4학년)

*  *  *

## Very Special Thanks to 한림 소프트웨어 중심대학 사업단

![X-banner](./x-banner.png)
